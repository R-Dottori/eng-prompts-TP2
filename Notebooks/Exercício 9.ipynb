{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exercício 9: Cálculo de Tokens em Texto Longo com API Gemini**\n",
    "\n",
    "*Implemente um notebook que use a API Gemini para calcular a quantidade de tokens necessários para processar um texto de 5.000 palavras.* \n",
    "\n",
    "*Baseie-se no modelo de tokenização utilizado por Gemini e explique como a quantidade de tokens influencia o custo e o desempenho da interação com LLMs em textos longos.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import requests\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "genai.configure(api_key=os.environ['GEMINI_KEY'])\n",
    "\n",
    "modelo = genai.GenerativeModel('gemini-1.5-flash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_principe = requests.get('https://mais-diferencas.s3.us-east-2.amazonaws.com/books/O+Pequeno+Pr%C3%ADncipe/original.txt')\n",
    "\n",
    "texto = resp_principe.text.split()[:5000]\n",
    "texto = ' '.join(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "codificador = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "tokens = codificador.encode(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8701"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46, 5250, 447]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos um trecho de 5.000 palavras do livro \"O Pequeno Príncipe\" e foram necessários **8.701 tokens para processá-lo**.\n",
    "\n",
    "Os *tokens* são as divisões usadas pelos LLMs para processar textos em representações numéricas. Podem ser palavras inteiras, radicais, pontuações, etc.\n",
    "\n",
    "Portanto, um vocabulário extenso resulta em um maior número de *tokens*, o que provavelmente necessitará de um tempo de processamento maior, possibilidade de se perder o contexto — tanto pelo número de palavras em si quanto caso haja a necessidade de separar o texto em pedaços menores — e uma tarefa mais custosa no geral, já que a maioria desses modelos cobra pelo número de *tokens*.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tp2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
